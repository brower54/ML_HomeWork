#!/usr/bin/env python
# coding: utf-8
# Кильдеев Заур, 4бПМ
# # Машинное обучение
# 
# Практическое задание 1 посвящено изучению основных библиотек для анализа данных, а также линейных моделей и методов их обучения. Вы научитесь:
#  * применять библиотеки NumPy и Pandas для осуществления желаемых преобразований;
#  * подготавливать данные для обучения линейных моделей;
#  * обучать линейную, Lasso и Ridge-регрессии при помощи модуля scikit-learn;
#  * реализовывать обычный и стохастический градиентные спуски;
#  * обучать линейную регрессию для произвольного функционала качества.

# ## Библиотеки для анализа данных
# 
# ### NumPy
# 
# Во всех заданиях данного раздела запрещено использовать циклы  и list comprehensions. Под вектором и матрицей в данных заданиях понимается одномерный и двумерный numpy.array соответственно.

# In[22]:


import numpy as np


# Реализуйте функцию, возвращающую максимальный элемент в векторе x среди элементов, перед которыми стоит нулевой. Для x = np.array([6, 2, 0, 3, 0, 0, 5, 7, 0]) ответом является 5. Если нулевых элементов нет, функция должна возвращать None.
# 

# In[23]:


def max_element(arr):
    arr = np.array([6, 2, 0, 3, 0, 0, 5, 8,])
    x = arr == 0
print ([1:][arr[:-1]].max())
x = np.array([0, 1, 0, 3, 0, 0, 5, 7, 0])
print(np.max(np.take(x, np.where(x[1:] == 0))))


# In[25]:


x = np.array([0, 1, 0, 3, 0, 0, 5, 7, 0])
print(np.max(np.take(x, np.where(x[:1] == 0))))


# Реализуйте функцию, принимающую на вход матрицу и некоторое число и возвращающую ближайший к числу элемент матрицы. Например: для X = np.arange(0,10).reshape((2, 5)) и v = 3.6 ответом будет 4.

# In[7]:


def nearest_value(X, v):
    X = X.ravel()
    print(X)
    idx = np.abs(X - v).argmin()
    return X[idx]

Z = np.arange(0,10).reshape((2, 5))
Z[0, 0] = 3
v = 2.2
print(Z)
print('Nearest value to {} is {}'.format(v, nearest_value(Z, v)))


# Реализуйте функцию scale(X), которая принимает на вход матрицу и масштабирует каждый ее столбец (вычитает выборочное среднее и делит на стандартное отклонение). Убедитесь, что в функции не будет происходить деления на ноль. Протестируйте на случайной матрице (для её генерации можно использовать, например, функцию [numpy.random.randint](http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randint.html)).

# In[25]:


def scale(X):
    eps = 0.0000001
    e = np.mean(X, axis=0)
    d = np.std(X, axis=0)
    is_not_zero = np.abs(d) > eps
    X[:, is_not_zero] = (X - e)[:, is_not_zero] / d[is_not_zero]
    return X
        
X = scale(np.array([[1, 2, 0], [1, 5, 0], [2, 8, 0], [2, 3, 1]], dtype=float))
print (X)
X = scale(np.array([[1, 2, 2], [1, 5, 2], [2, 8, 2], [2, 3, 2]], dtype=float))
print (X)


#  Реализуйте функцию, которая для заданной матрицы находит:
#  - определитель
#  - след
#  - наименьший и наибольший элементы
#  - норму Фробениуса
#  - собственные числа
#  - обратную матрицу
# 
# Для тестирования сгенерируйте матрицу с элементами из нормального распределения $\mathcal{N}$(10,1)

# In[9]:


from numpy import linalg as la
def get_stats(matrix):
    matrix = np.random.normal(10, 1, (10, 10))
print(la.det(matrix))
print(np.trace(matrix))
print(np.min(matrix))
print(np.max(matrix))
print(la.norm(matrix, ord=2))
print(la.norm(matrix, ord='fro'))
print(la.eig(matrix)[0])
print(la.inv(matrix))


# Повторите 100 раз следующий эксперимент: сгенерируйте две матрицы размера 10×10 из стандартного нормального распределения, перемножьте их (как матрицы) и найдите максимальный элемент. Какое среднее значение по экспериментам у максимальных элементов? 95-процентная квантиль?

# In[22]:


values = np.empty(100)
for exp_num in range(100):
    m1 = np.random.normal(0, 1, (10, 10))
    m2 = np.random.normal(0, 1, (10, 10))
    values[exp_num] = np.max(m1.dot(m2))
print(np.average(values))
print(np.percentile(values, 95))


# ### Pandas
# 
# ![](https://metrouk2.files.wordpress.com/2015/10/panda.jpg)
# 
# #### Ответьте на вопросы о данных по авиарейсам в США за январь-апрель 2008 года.
# 
# [Данные](https://www.dropbox.com/s/dvfitn93obn0rql/2008.csv?dl=0) и их [описание](http://stat-computing.org/dataexpo/2009/the-data.html)

# In[5]:


import pandas as pd
get_ipython().run_line_magic('matplotlib', 'inline')
data = pd.read_csv(r"C:\Users\Заур\Downloads\2008.csv", error_bad_lines=False)
data.head()


# Какая из причин отмены рейса (`CancellationCode`) была самой частой? (расшифровки кодов можно найти в описании данных)

# In[7]:


codes = data["CancellationCode"]
codes.value_counts() #A = carrier


#  Найдите среднее, минимальное и максимальное расстояние, пройденное самолетом.

# In[12]:


dists = data["Distance"]
print ("Max:", dists.max())
print ("Average: ", dists.mean())
print ("Min: ", dists.min())


#  Не выглядит ли подозрительным минимальное пройденное расстояние? В какие дни и на каких рейсах оно было? Какое расстояние было пройдено этими же рейсами в другие дни?

# In[26]:


date_fnum_ucarrier = ["Year", "Month", "DayofMonth", "FlightNum", "UniqueCarrier"]
fnum_ucarrier_dist = ["FlightNum", "UniqueCarrier", "Distance"]
days = data[dists == dists.min()][date_fnum_ucarrier].drop_duplicates()

print (days)

flights = data[dists == dists.min()][["FlightNum", "UniqueCarrier"]].drop_duplicates()
get_other_flights = lambda flight:     np.logical_and(data["FlightNum"] == flight[0], data["UniqueCarrier"] == flight[1])

[data[get_other_flights(flight)][fnum_ucarrier_dist].drop_duplicates() for flight in flights.values]


# Из какого аэропорта было произведено больше всего вылетов? В каком городе он находится?

# In[16]:


airports = pd.read_csv("http://stat-computing.org/dataexpo/2009/airports.csv")
most_frequent_airport = data["Origin"].value_counts().index[0]
airports[airports["iata"] == most_frequent_airport]


# Найдите для каждого аэропорта среднее время полета (`AirTime`) по всем вылетевшим из него рейсам. Какой аэропорт имеет наибольшее значение этого показателя?

# In[18]:


origin_mean_airtime = data.groupby("Origin")["AirTime"].aggregate(np.mean)
print (origin_mean_airtime.max())
airport_max = origin_mean_airtime.idxmax()
airports[airports["iata"] == airport_max]


# Найдите аэропорт, у которого наибольшая доля задержанных (`DepDelay > 0`) рейсов. Исключите при этом из рассмотрения аэропорты, из которых было отправлено меньше 1000 рейсов (используйте функцию `filter` после `groupby`).

# In[20]:


threshold = 1000
all_flights = data.groupby("Origin").size()
delayed_flights = data[data["DepDelay"] > 0].groupby("Origin").size()
fraction_delayed = delayed_flights[all_flights > threshold] / all_flights[all_flights > threshold]
max_fraction_delayed = fraction_delayed.idxmax()
print (fraction_delayed.max())
airports[airports["iata"] == max_fraction_delayed]


# ## Линейная регрессия
# 
# В этой части мы разберемся с линейной регрессией, способами её обучения и измерением качества ее прогнозов. 
# 
# Будем рассматривать датасет из предыдущей части задания для предсказания времени задержки отправления рейса в минутах (DepDelay). Отметим, что под задержкой подразумевается не только опоздание рейса относительно планируемого времени вылета, но и отправление до планируемого времени.
# 
# ### Подготовка данных
# 
# **12. (0.5 балла)** Считайте выборку из файла при помощи функции pd.read_csv и ответьте на следующие вопросы:
#    - Имеются ли в данных пропущенные значения?
#    - Сколько всего пропущенных элементов в таблице "объект-признак"?
#    - Сколько объектов имеют хотя бы один пропуск?
#    - Сколько признаков имеют хотя бы одно пропущенное значение?

# In[ ]:


# Your code here


# Как вы понимаете, также не имеет смысла рассматривать при решении поставленной задачи объекты с пропущенным значением целевой переменной. В связи с этим ответьте на следующие вопросы и выполните соответствующие действия:
# - Имеются ли пропущенные значения в целевой переменной?
# - Проанализируйте объекты с пропущенными значениями целевой переменной. Чем вызвано это явление? Что их объединяет? Можно ли в связи с этим, на ваш взгляд, исключить какие-то признаки из рассмотрения? Обоснуйте свою точку зрения.
# 
# Исключите из выборки объекты **с пропущенным значением целевой переменной и со значением целевой переменной, равным 0**, а также при необходимости исключите признаки в соответствии с вашим ответом на последний вопрос из списка и выделите целевую переменную в отдельный вектор, исключив её из матрицы "объект-признак".

# In[ ]:


# Your code here


# **13. (0.5 балла)** Обратите внимание, что признаки DepTime, CRSDepTime, ArrTime, CRSArrTime приведены в формате hhmm, в связи с чем будет не вполне корректно рассматривать их как вещественные.
# 
# Преобразуйте каждый признак FeatureName из указанных в пару новых признаков FeatureName\_Hour, FeatureName\_Minute, разделив каждое из значений на часы и минуты. Не забудьте при этом исключить исходный признак из выборки. В случае, если значение признака отсутствует, значения двух новых признаков, его заменяющих, также должны отсутствовать. 
# 
# Например, признак DepTime необходимо заменить на пару признаков DepTime_Hour, DepTime_Minute. При этом, например, значение 155 исходного признака будет преобразовано в значения 1 и 55 признаков DepTime_Hour, DepTime_Minute соответственно.

# In[ ]:


# Your code here


# **14. (0.5 балла)** Некоторые из признаков, отличных от целевой переменной, могут оказывать чересчур значимое влияние на прогноз, поскольку по своему смыслу содержат большую долю информации о значении целевой переменной. Изучите описание датасета и исключите признаки, сильно коррелирующие с ответами. Ваш выбор признаков для исключения из выборки обоснуйте. Кроме того, исключите признаки TailNum и Year.

# In[ ]:


# Your code here


# **15. (1 балл)** Приведем данные к виду, пригодному для обучения линейных моделей. Для этого вещественные признаки надо отмасштабировать, а категориальные — привести к числовому виду. Также надо устранить пропуски в данных.

# В первую очередь поймем, зачем необходимо применять масштабирование. Следующие ячейки с кодом построят гистограммы для 3 вещественных признаков выборки.

# In[ ]:


X['DepTime_Hour'].hist(bins=20)


# In[ ]:


X['TaxiIn'].hist(bins=20)


# In[ ]:


X['FlightNum'].hist(bins=20)


# Какую проблему вы наблюдаете на этих графиках? Как масштабирование поможет её исправить?

# Некоторые из признаков в нашем датасете являются категориальными. Типичным подходом к работе с ними является бинарное, или [one-hot-кодирование](https://en.wikipedia.org/wiki/One-hot).
# 
# Реализуйте функцию transform_data, которая принимает на вход DataFrame с признаками и выполняет следующие шаги:
# 1. Замена пропущенных значений на нули для вещественных признаков и на строки 'nan' для категориальных.
# 2. Масштабирование вещественных признаков с помощью [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).
# 3. One-hot-кодирование категориальных признаков с помощью [DictVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) или функции [pd.get_dummies](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html).
# 
# Метод должен возвращать преобразованный DataFrame, который должна состоять из масштабированных вещественных признаков и закодированных категориальных (исходные признаки должны быть исключены из выборки).

# In[ ]:


def transform_data(data):
    # Your code here


# Примените функцию transform_data к данным. Сколько признаков получилось после преобразования?

# In[ ]:


# Your code here


# **16. (0.5 балла)** Разбейте выборку и вектор целевой переменной на обучение и контроль в отношении 70/30 (для этого можно использовать, например, функцию [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html)). 

# In[ ]:


# Your code here


# ### Scikit-learn
# 
# <img src = "https://pp.vk.me/c4534/u35727827/93547647/x_d31c4463.jpg">
# Теперь, когда мы привели данные к пригодному виду, попробуем решить задачу при помощи метода наименьших квадратов. Напомним, что данный метод заключается в оптимизации функционала $MSE$:
# 
# $$MSE(X, y) = \frac{1}{l} \sum_{i=1}^l (<w, x_i> - y_i)^2 \to \min_{w},$$
# 
# где $\{ (x_i, y_i ) \}_{i=1}^l$ — обучающая выборка, состоящая из $l$ пар объект-ответ.
# 
# Заметим, что решение данной задачи уже реализовано в модуле sklearn в виде класса [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression).
# 
# **17. (0.5 балла)** Обучите линейную регрессию на 1000 объектах из обучающей выборки и выведите значения $MSE$ и $R^2$ на этой подвыборке и контрольной выборке (итого 4 различных числа). Проинтерпретируйте полученный результат — насколько качественные прогнозы строит полученная модель? Какие проблемы наблюдаются в модели?
# 
# **Подсказка**: изучите значения полученных коэффициентов $w$, сохраненных в атрибуте coef_ объекта LinearRegression.

# In[ ]:


# Your code here


# Для решения описанных вами в предыдущем пункте проблем используем L1- или L2-регуляризацию, тем самым получив Lasso и Ridge регрессии соответственно и изменив оптимизационную задачу одним из следующих образов:
# $$MSE_{L1}(X, y) = \frac{1}{l} \sum_{i=1}^l (<w, x_i> - y_i)^2 + \alpha ||w||_1 \to \min_{w},$$
# $$MSE_{L2}(X, y) = \frac{1}{l} \sum_{i=1}^l (<w, x_i> - y_i)^2 + \alpha ||w||_2^2 \to \min_{w},$$
# 
# где $\alpha$ — коэффициент регуляризации. Один из способов его подбора заключается в переборе некоторого количества значений и оценке качества на кросс-валидации для каждого из них, после чего выбирается значение, для которого было получено наилучшее качество.
# 
# **18. (0.5 балла)** Обучите линейные регрессии с L1- и L2-регуляризатором, подобрав лучшее значение параметра регуляризации из списка alpha_grid при помощи кросс-валидации c 5 фолдами на тех же 1000 объектах, что и в п.17. Выведите значения $MSE$ и $R^2$ на обучающей и контрольной выборках. Удалось ли решить указанные вами ранее проблемы?
# 
# Для выполнения данного задания вам могут понадобиться реализованные в библиотеке объекты [LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html), [RidgeCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) и [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html).
# 

# In[ ]:


# Your code here


# ### Градиентный спуск
# 
# В предыдущем разделе мы использовали существующие реализации методов обучения линейной регрессии с регуляризацией и без. Тем не менее, подобные реализации, как правило, имеются лишь для ограниченного набора стандартных методов. В частности, при выходе функционала качества за пределы стандартного множества необходимо самостоятельно реализовывать составляющие процесса решения оптимизационной задачи. Именно этому и посвящен данный раздел задания.
# 
# Пусть необходимо минимизировать следующий функционал (Mean Square Percentage Error — модифицированный [RMSPE](https://www.kaggle.com/c/rossmann-store-sales/details/evaluation)):
# $$MSPE(\{x_i, y_i\}_{i=1}^l, \, w) = \frac{1}{l}\sum_{i=1}^l \left( \frac{y_i - \langle w, x_i \rangle }{y_i} \right)^2,$$
# 
# где $\{x_i, y_i\}_{i=1}^l$ — обучающая выборка, $w$ — вектор весов линейной модели. Будем также рассматривать функционал $MSPE$ с L2-регуляризацией:
# 
# $$MSPE(\{x_i, y_i\}_{i=1}^l, \, w) = \frac{1}{l}\sum_{i=1}^l \left( \frac{y_i - \langle w, x_i \rangle }{y_i} \right)^2 + ||w||_2^2.$$
# 
# **19. (0 баллов)** Добавьте к объектам обеих выборок из п. 16 единичный признак.

# In[ ]:


# Your code here


# **20. (1 балл)** Реализуйте функции, которые вычисляют:
#  * прогнозы линейной модели;
#  * функционал $MSPE$ и его градиент;
#  * регуляризованный $MSPE$ и его градиент.

# In[ ]:


# возвращает вектор прогнозов линейной модели с вектором весов w для выборки X
def make_pred(X, w):
    pass


# In[ ]:


# возвращает значение функционала MSPE для выборки (X, y) и вектора весов w
def get_func(w, X, y):
    pass


# In[ ]:


# возвращает градиент функционала MSPE для выборки (X, y) и вектора весов w
def get_grad(w, X, y):
    pass


# In[ ]:


# возвращает значение регуляризованного функционала MSPE для выборки (X, y) и вектора весов w
def get_reg_func(w, X, y):
    pass


# In[ ]:


# возвращает градиент регуляризованного функционала MSPE для выборки (X, y) и вектора весов w
def get_reg_grad(w, X, y):
    pass


# **21. (1 балл)** Реализуйте метод градиентного спуска для описанных функционалов ($MSPE$ и его регуляризованный вариант). Функция должна принимать следующие параметры:
#  - X — матрица "объект-признак";
#  - y — вектор целевой переменной;
#  - w0 — начальное значение вектора весов;
#  - step_size — значение темпа обучения;
#  - max_iter — максимальное число итераций;
#  - eps — значение, используемое в критерии останова;
#  - is_reg — бинарный параметр, принимает значение True в случае наличия регуляризации функционала, False — в противном случае.
#  
# Процесс должен быть остановлен, если выполнено хотя бы одно из следующих условий:
#  - было выполнено заданное количество итераций max_iter;
#  - евклидова норма разности векторов $w$ на соседних итерациях стала меньше, чем eps.
# 
# Функция должна возвращать полученный в результате оптимизации вектор $w$ и список значений функционала на каждой итерации.

# In[ ]:


def grad_descent(X, y, step_size, max_iter, eps, is_reg):
    # Your code here


# Обучите линейную регрессию с функционалом $MSPE$ на обучающей выборке при помощи метода градиентного спуска и изобразите кривые зависимости значения функционала от номера итерации для различных:
#  * значений размера шага из набора [0.001, 1, 10];
#  * способов начальной инициализации вектора весов (нули, случайные веса).
# 
# Проанализируйте полученные результаты — влияют ли данные параметры на скорость сходимости и итоговое качество? Если да, то как?

# In[ ]:


# Your code here


# **22. (0.5 балла)** Обучите линейную регрессию с функционалом MSPE и его регуляризованным вариантом на обучающей выборке при помощи метода градиентного спуска и изобразите кривые зависимости значения функционала от номера итерации. Исследуйте зависимость скорости сходимости от наличия регуляризации. Обоснуйте, почему так происходит.

# In[ ]:


# Your code here


# Метод градиентного спуска может быть весьма трудозатратен в случае большого размера обучающей выборки. Поэтому часто используют метод стохастического градиентного спуска, где на каждой итерации выбирается случайный объект из обучающей выборки и обновление весов происходит только по этому объекту. 
# 
# **23. (1 доп. балл)**  Реализуйте метод стохастического градиентного спуска (SGD) для описанных функционалов ($MSPE$ и его регуляризованный вариант). Функция должна иметь параметры и возвращаемое значение, аналогичные оным функции grad\_descent из п.21. Кроме того, должен использоваться аналогичный критерий останова.

# In[ ]:


def sgd(X, y, step_size, max_iter, eps, is_reg):
    # Your code here


# Обучите линейную регрессию с функционалом $MSPE$ и его регуляризованным вариантом на обучающей выборке при помощи метода стохастического градиентного спуска, подобрав при этом размер шага, при котором метод будет сходиться. Нарисуйте график сходимости. Выведите значения $MSPE, MSE, R^2$ на контрольной выборке.

# In[ ]:


# Your code here


# **24. (0.5 доп. балла)** Аналогично п.22 исследуйте зависимость скорости сходимости метода SGD от наличия регуляризации. Обоснуйте, почему так происходит.

# In[ ]:


# Your code here


# **25. (0.5 балла)** Обучите стандартную линейную регрессию с функционалом качества MSE на обучающей выборке и выведите значение MSPE полученного решения на контрольной выборке. Как оно соотносится с аналогичным результатом для решения, полученного в п.22? Почему?

# In[ ]:


# Your code here


# Здесь вы можете поделиться своими мыслями по поводу этого задания.

# In[ ]:





# А здесь — вставить вашу любимую картинку.

# In[ ]:




